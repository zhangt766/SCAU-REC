# -*- coding: utf-8 -*-
"""data_interface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQB1U9pJpUMEf07WrtsMv3wp7G-_RIYl
"""

import inspect
import importlib
import pytorch_lightning as pl
from torch.utils.data import DataLoader

import random
import torch
import os


class TrainCollater:
    """
    Collate function that:
    - fills [HistoryHere] / [CansHere] using seq_name / cans_name
    - produces tokenized inputs (pair format for training, single for eval)
    - packs tensors needed for embedding injection: seq/cans/item_id/len_seq/len_cans
    - (NEW) passes through user_id / t if present in samples (for user-wise/streaming eval)
    """

    def __init__(self,
                 prompt_list=None,
                 llm_tokenizer=None,
                 train=False,
                 terminator="\n",
                 max_step=1):
        self.prompt_list = prompt_list
        self.llm_tokenizer = llm_tokenizer
        self.train = train
        self.terminator = terminator
        self.max_step = max_step
        self.cur_step = 1

    def _pick_instruction(self, batch_size: int):
        """
        FIX: the old code referenced `sample` when prompt_list is not list.
        We make instruction selection robust:
        - if prompt_list is a non-empty list: sample one prompt and broadcast
        - else: use empty string prompts (or you can default to a hard-coded template)
        """
        if isinstance(self.prompt_list, list) and len(self.prompt_list) > 0:
            instruction = random.choice(self.prompt_list)
            return [instruction] * batch_size
        else:
            return [""] * batch_size

    def __call__(self, batch):
        batch_size = len(batch)
        inputs_text = self._pick_instruction(batch_size)

        # teacher-forcing curriculum toggle (your original logic)
        thresh_hold = self.cur_step / max(1, self.max_step)
        p = random.random()

        use_placeholder = (p < thresh_hold) if self.train else False  # train may use [PH], eval never
        flag = False

        for i, sample in enumerate(batch):
            input_text = inputs_text[i]

            # Safety: if a dataset doesn't provide seq_name/cans_name, fall back to empty lists
            seq_names = sample.get("seq_name", [])
            cans_names = sample.get("cans_name", [])

            if "[HistoryHere]" in input_text:
                if not use_placeholder:
                    insert_prompt = ", ".join([f"{title} [HistoryEmb]" for title in seq_names])
                else:
                    insert_prompt = ", ".join([f"{title} [PH]" for title in seq_names])
                    flag = True
                input_text = input_text.replace("[HistoryHere]", insert_prompt)

            if "[CansHere]" in input_text:
                if not use_placeholder:
                    insert_prompt = ", ".join([f"{title} [CansEmb]" for title in cans_names])
                else:
                    insert_prompt = ", ".join([f"{title} [PH]" for title in cans_names])
                    flag = True
                input_text = input_text.replace("[CansHere]", insert_prompt)

            inputs_text[i] = input_text

        self.cur_step += 1

        # targets_text is used only for training tokenization
        targets_text = [sample.get("correct_answer", "") for sample in batch]

        # ---- Pack tensors ----
        # seq/cans/len_seq/len_cans/item_id MUST exist for your embedding injection
        seq = torch.stack([torch.tensor(sample["seq"]) for sample in batch], dim=0)
        cans = torch.stack([torch.tensor(sample["cans"]) for sample in batch], dim=0)
        len_seq = torch.stack([torch.tensor(sample["len_seq"]) for sample in batch], dim=0)
        len_cans = torch.stack([torch.tensor(sample["len_cans"]) for sample in batch], dim=0)
        item_id = torch.stack([torch.tensor(sample["item_id"]) for sample in batch], dim=0)

        # (NEW) Optional: user_id / t (for user-wise + streaming evaluation)
        # We keep them if dataset provides them; otherwise ignore.
        has_user = all(("user_id" in s) for s in batch)
        has_t = all(("t" in s) for s in batch)

        if self.train:
            # Training: pair tokenization [prompt, target]
            targets_text = [t + self.terminator for t in targets_text]
            inputs_pair = [[p, t] for p, t in zip(inputs_text, targets_text)]

            batch_tokens = self.llm_tokenizer(
                inputs_pair,
                return_tensors="pt",
                padding="longest",
                truncation=False,
                add_special_tokens=True,
                return_attention_mask=True,
                return_token_type_ids=True
            )

            new_batch = {
                "tokens": batch_tokens,
                "seq": seq,
                "cans": cans,
                "len_seq": len_seq,
                "len_cans": len_cans,
                "item_id": item_id,
                "flag": flag,
            }

        else:
            # Eval: single tokenization [prompt]
            batch_tokens = self.llm_tokenizer(
                inputs_text,
                return_tensors="pt",
                padding="longest",
                truncation=False,
                add_special_tokens=True,
                return_attention_mask=True
            )
            cans_name = [sample.get("cans_name", []) for sample in batch]

            new_batch = {
                "tokens": batch_tokens,
                "seq": seq,
                "cans": cans,
                "len_seq": len_seq,
                "len_cans": len_cans,
                "item_id": item_id,
                "correct_answer": targets_text,  # keep for debug; ranking doesn't need it
                "cans_name": cans_name,
            }

        if has_user:
            new_batch["user_id"] = torch.stack([torch.tensor(sample["user_id"]) for sample in batch], dim=0)
        if has_t:
            new_batch["t"] = torch.stack([torch.tensor(sample["t"]) for sample in batch], dim=0)

        return new_batch


class DInterface(pl.LightningDataModule):

    def __init__(self,
                 llm_tokenizer=None,
                 num_workers=8,
                 dataset='',
                 **kwargs):
        super().__init__()
        self.num_workers = num_workers
        self.llm_tokenizer = llm_tokenizer
        self.dataset = dataset
        self.kwargs = kwargs
        self.batch_size = kwargs['batch_size']
        self.max_epochs = kwargs['max_epochs']

        self.load_data_module()
        self.load_prompt(kwargs.get('prompt_path', ''))

        self.trainset = self.instancialize(stage='train')
        self.valset = self.instancialize(stage='val')
        self.testset = self.instancialize(stage='test')

        # NOTE: your old formula divided by num_workers; that is not correct for global steps.
        # But we keep it minimal: only avoid zero division.
        self.max_steps = max(1, self.max_epochs * max(1, (len(self.trainset) // self.batch_size)))

    def train_dataloader(self):
        return DataLoader(
            self.trainset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=True,
            drop_last=True,
            collate_fn=TrainCollater(
                prompt_list=self.prompt_list,
                llm_tokenizer=self.llm_tokenizer,
                train=True,
                max_step=self.max_steps
            )
        )

    def val_dataloader(self):
        return DataLoader(
            self.valset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            collate_fn=TrainCollater(
                prompt_list=self.prompt_list,
                llm_tokenizer=self.llm_tokenizer,
                train=False
            )
        )

    def test_dataloader(self):
        return DataLoader(
            self.testset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            collate_fn=TrainCollater(
                prompt_list=self.prompt_list,
                llm_tokenizer=self.llm_tokenizer,
                train=False
            )
        )

    def load_data_module(self):
        name = self.dataset
        camel_name = ''.join([i.capitalize() for i in name.split('_')])
        try:
            self.data_module = getattr(importlib.import_module('.' + name, package=__package__), camel_name)
        except Exception as e:
            raise ValueError(f'Invalid Dataset File Name or Invalid Class Name data.{name}.{camel_name}') from e

    def instancialize(self, **other_args):
        class_args = inspect.getargspec(self.data_module.__init__).args[1:]
        inkeys = self.kwargs.keys()
        args1 = {}
        for arg in class_args:
            if arg in inkeys:
                args1[arg] = self.kwargs[arg]
        args1.update(other_args)
        return self.data_module(**args1)

    def load_prompt(self, prompt_path):
        if prompt_path and os.path.isfile(prompt_path):
            with open(prompt_path, 'r') as f:
                raw_prompts = f.read().splitlines()
            self.prompt_list = [p.strip() for p in raw_prompts if p.strip()]
            print(f'Load {len(self.prompt_list)} training prompts')
            if len(self.prompt_list) > 0:
                print('Prompt Example \n{}'.format(random.choice(self.prompt_list)))
        else:
            self.prompt_list = []