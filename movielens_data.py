# -*- coding: utf-8 -*-
"""movielens_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MWIATDgANBIaTrM163Fzi-I7lbWDuenu
"""

import os.path as op
import random
import pandas as pd
import torch.utils.data as data


class MovielensData(data.Dataset):
    """
    Reference dataset implementation for LLM-based sequential recommendation
    with candidate ranking.

    Each instance is constructed as: (user history -> next item) with a fixed-size
    candidate set for ranking / candidate-restricted softmax training.

    Returned sample schema:
      - history_ids: List[int]
      - history_text: List[str]
      - target_id: int
      - candidates: List[int]          # contains target_id
      - candidate_text: List[str]
      - target_pos: int                # index of target_id in candidates
      - history_str / candidates_str: str (optional, for prompt templates)
      - split: str ("train"/"val"/"test")
    """

    def __init__(
        self,
        data_dir=r"data/ref/movielens",
        stage="train",
        cans_num=10,
        sep=", ",
        no_augment=True,
        seed=42,
    ):
        self.data_dir = data_dir
        self.stage = stage
        self.cans_num = int(cans_num)
        self.sep = sep
        self.no_augment = no_augment

        # Keep the original padding id used in your preprocessing
        self.padding_item_id = 4606

        # For reproducible negative sampling (optional but reviewer-friendly)
        self.rng = random.Random(seed)

        self.check_files()

    def __len__(self):
        return len(self.session_data)

    def __getitem__(self, i):
        row = self.session_data.iloc[i]

        history_ids = row["seq_unpad"]
        history_text = row["seq_title"]
        target_id = int(row["next"])
        target_text = row["next_item_name"]

        candidates = self.negative_sampling(history_ids, target_id)
        candidate_text = [self.item_id2name[c] for c in candidates]
        target_pos = candidates.index(target_id)

        sample = {
            # core fields for ranking
            "history_ids": history_ids,
            "history_text": history_text,
            "target_id": target_id,
            "target_text": target_text,
            "candidates": candidates,
            "candidate_text": candidate_text,
            "target_pos": target_pos,
            "split": self.stage,
            # optional convenience fields for prompt templates
            "history_str": self.sep.join(history_text),
            "candidates_str": self.sep.join(candidate_text),
            "user_id": temp["user_id"],
            "t": temp["t"]

        }
        return sample

    def negative_sampling(self, history_ids, target_id):
        """
        Sample (cans_num-1) negatives uniformly from the item universe,
        excluding items in history and excluding the target.
        Then append the target and shuffle.
        """
        history_set = set(history_ids)

        # item universe
        all_items = list(self.item_id2name.keys())

        # candidate pool excludes history + target
        pool = [it for it in all_items if (it not in history_set) and (it != target_id)]
        if len(pool) < self.cans_num - 1:
            # fallback: in rare cases pool may be too small
            negs = pool
        else:
            negs = self.rng.sample(pool, self.cans_num - 1)

        candidates = negs + [target_id]
        self.rng.shuffle(candidates)
        return candidates

    def check_files(self):
        self.item_id2name = self.get_movie_id2name()

        if self.stage == "train":
            filename = "similar_train_data.df"
        elif self.stage == "val":
            filename = "similar_val_data.df"
        elif self.stage == "test":
            filename = "similar_test_data.df"
        else:
            raise ValueError(f"Unknown stage: {self.stage}")

        data_path = op.join(self.data_dir, filename)
        self.session_data = self.session_data4frame(data_path, self.item_id2name)

    def get_movie_id2name(self):
        movie_id2name = {}
        item_path = op.join(self.data_dir, "id2name.txt")
        with open(item_path, "r") as f:
            for l in f.readlines():
                ll = l.strip("\n").split("::")
                movie_id2name[int(ll[0])] = ll[1].strip()
        return movie_id2name

    def session_data4frame(self, datapath, movie_id2name):
        df = pd.read_pickle(datapath)

        # keep only sequences with sufficient length (as in your original code)
        df = df[df["len_seq"] >= 3].copy()

        def remove_padding(seq):
            # remove padding tokens (max 10 removals as in your original)
            x = list(seq)
            for _ in range(10):
                try:
                    x.remove(self.padding_item_id)
                except ValueError:
                    break
            return x

        df["seq_unpad"] = df["seq"].apply(remove_padding)

        def seq_to_title(seq_ids):
            titles = []
            for it in seq_ids:
                if it in movie_id2name:
                    titles.append(movie_id2name[it])
                else:
                    titles.append("Unknown")
            return titles

        df["seq_title"] = df["seq_unpad"].apply(seq_to_title)

        def next_item_title(it):
            return movie_id2name.get(it, "Unknown")

        df["next_item_name"] = df["next"].apply(next_item_title)

        # IMPORTANT: we intentionally do not expose AdaptRec-specific fields
        # (e.g., most_similar_seq*) in this reference dataset.

        return df.reset_index(drop=True)