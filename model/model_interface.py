# -*- coding: utf-8 -*-
"""model_interface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q_cohTMrJRVfawhRh-z9jGme7LffCTgt
"""

import inspect
import torch
import importlib
from torch.nn import functional as F

import pytorch_lightning as pl
import pandas as pd

from transformers import LlamaForCausalLM, LlamaTokenizer
import os.path as op
import os
import numpy as np

from optims import LinearWarmupCosineLRScheduler
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
from clara_stream_eval import run_streaming_eval


class MInterface(pl.LightningModule):
    """
    CLARA-ready refactor (B):
    - keep training exactly the same (LM loss)
    - replace generate/HR@1 evaluation with streaming ranking evaluation:
        Hit@K / NDCG@K / MRR@K
    - delegate streaming logic to clara_stream_eval.py
    """

    def __init__(self, **kargs):
        super().__init__()
        self.save_hyperparameters()

        self.load_llm(self.hparams.llm_path)
        self.load_rec_model(self.hparams.rec_model_path)
        self.load_projector()

        # eval config (with safe defaults)
        self.eval_ks = getattr(self.hparams, "eval_ks", [5, 10])
        self.eval_temperature = float(getattr(self.hparams, "eval_temperature", 1.0))

    # -------------------- forward / loss --------------------
    def forward(self, batch):
        targets = batch["tokens"].input_ids.masked_fill(
            batch["tokens"].input_ids == self.llama_tokenizer.pad_token_id, -100
        )


        if hasattr(batch["tokens"], "token_type_ids"):
            targets = targets.masked_fill((batch["tokens"].token_type_ids == 0)[:, 1:], -100)

        input_embeds = self.wrap_emb(batch)

        outputs = self.llama_model(
            inputs_embeds=input_embeds,
            attention_mask=batch["tokens"].attention_mask,
            return_dict=True,
            labels=targets,
            use_cache=False
        )
        return outputs

    def configure_loss(self, out, labels=None):
        loss = self.hparams.loss.lower()
        if loss == 'lm':
            return out.loss
        raise ValueError("Invalid Loss Type!")

    # -------------------- training --------------------
    def training_step(self, batch, batch_idx):
        if getattr(self, "scheduler", None):
            self.scheduler.step(self.trainer.global_step, self.current_epoch, self.trainer.max_steps)

        # keep your original projector freezing behavior
        if batch.get("flag", False):
            for _, param in self.projector.named_parameters():
                param.requires_grad = False
        else:
            for _, param in self.projector.named_parameters():
                param.requires_grad = True

        out = self(batch)
        loss = self.configure_loss(out)

        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True)

        if getattr(self, "scheduler", None):
            self.log('lr', self.scheduler.optimizer.param_groups[0]['lr'], on_step=True, on_epoch=True, prog_bar=True)

        self.log('global_step_num', self.trainer.global_step, on_step=True, on_epoch=True, prog_bar=True)
        return loss

    # -------------------- validation / test (CLARA streaming ranking) --------------------
    def on_validation_epoch_end(self):
        os.makedirs(self.hparams.output_dir, exist_ok=True)

        # ⚠ Important: requires dataloader shuffle=False and batches include user_id, t
        dl = self.trainer.datamodule.val_dataloader()
        scored_df, metrics = run_streaming_eval(
            model=self,
            dataloader=dl,
            ks=self.eval_ks,
            temperature=self.eval_temperature,
            output_csv_path=op.join(self.hparams.output_dir, "valid_scored.csv"),
            require_user_and_time=True,  # set False temporarily if you haven't added them yet
        )

        for k, v in metrics.items():
            self.log(f"val_{k}", round(float(v), 4), prog_bar=True)

    def on_test_epoch_end(self):
        os.makedirs(self.hparams.output_dir, exist_ok=True)

        dl = self.trainer.datamodule.test_dataloader()
        scored_df, metrics = run_streaming_eval(
            model=self,
            dataloader=dl,
            ks=self.eval_ks,
            temperature=self.eval_temperature,
            output_csv_path=op.join(self.hparams.output_dir, "test_scored.csv"),
            require_user_and_time=True,
        )

        for k, v in metrics.items():
            self.log(f"test_{k}", round(float(v), 4), prog_bar=True)

    # -------------------- optimizers --------------------
    def configure_optimizers(self):
        weight_decay = getattr(self.hparams, "weight_decay", 0)

        optimizer = torch.optim.Adam([
            {'params': self.projector.parameters(), 'lr': self.hparams.lr, 'weight_decay': weight_decay},
            {'params': self.llama_model.parameters(), 'lr': self.hparams.lr}
        ])

        if self.hparams.lr_scheduler is None:
            return optimizer

        max_step = self.trainer.max_steps
        warmup_steps = max_step // 20
        print(f'max_step: {max_step}')
        print(f'warmup_steps: {warmup_steps}')

        if self.hparams.lr_scheduler == 'cosine':
            self.scheduler = LinearWarmupCosineLRScheduler(
                optimizer,
                max_step=max_step,
                min_lr=self.hparams.lr_decay_min_lr,
                init_lr=self.hparams.lr,
                warmup_steps=warmup_steps,
                warmup_start_lr=self.hparams.lr_warmup_start_lr
            )
        else:
            self.scheduler = None
            raise ValueError('Invalid lr_scheduler type!')

        return optimizer

    def on_save_checkpoint(self, checkpoint):
        if self.hparams.save == 'part':
            checkpoint.pop('optimizer_states', None)
            to_be_removed = []
            for key in list(checkpoint['state_dict'].keys()):
                try:
                    if not self.get_parameter(key).requires_grad:
                        to_be_removed.append(key)
                except AttributeError:
                    to_be_removed.append(key)
            for key in to_be_removed:
                checkpoint['state_dict'].pop(key, None)

    # -------------------- model loading --------------------
    def load_llm(self, llm_path):
        print('Loading LLAMA')
        self.llama_tokenizer = LlamaTokenizer.from_pretrained(llm_path, use_fast=False)
        self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token
        self.llama_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        self.llama_tokenizer.padding_side = "right"
        self.llama_tokenizer.add_special_tokens(
            {'additional_special_tokens': ['[PH]', '[HistoryEmb]', '[CansEmb]', '[ItemEmb]']}
        )

        self.llama_model = LlamaForCausalLM.from_pretrained(llm_path, torch_dtype=torch.bfloat16)
        self.llama_model.resize_token_embeddings(len(self.llama_tokenizer))

        if self.hparams.llm_tuning == 'lora':
            if getattr(self.hparams, "peft_dir", None):
                # ✅ fix bug: self.llm_model does not exist
                self.llama_model = PeftModel.from_pretrained(
                    self.llama_model, self.hparams.peft_dir, is_trainable=True
                )
            else:
                if getattr(self.hparams, "peft_config", None):
                    peft_config = LoraConfig(**LoraConfig.from_json_file(self.hparams.peft_config))
                else:
                    peft_config = LoraConfig(
                        task_type=TaskType.CAUSAL_LM,
                        inference_mode=False,
                        r=self.hparams.lora_r,
                        lora_alpha=self.hparams.lora_alpha,
                        lora_dropout=self.hparams.lora_dropout,
                        target_modules=['k_proj', 'v_proj', 'q_proj', 'o_proj',
                                        'gate_proj', 'up_proj', 'down_proj']
                    )
                self.peft_config = peft_config
                self.llama_model = get_peft_model(self.llama_model, peft_config)

            self.llama_model.print_trainable_parameters()

        elif self.hparams.llm_tuning == 'freeze':
            for _, param in self.llama_model.named_parameters():
                param.requires_grad = False

        elif self.hparams.llm_tuning == 'freeze_lora':
            if getattr(self.hparams, "peft_dir", None):
                self.llama_model = PeftModel.from_pretrained(
                    self.llama_model, self.hparams.peft_dir, is_trainable=True
                )
            else:
                if getattr(self.hparams, "peft_config", None):
                    peft_config = LoraConfig(**LoraConfig.from_json_file(self.hparams.peft_config))
                else:
                    peft_config = LoraConfig(
                        task_type=TaskType.CAUSAL_LM,
                        inference_mode=False,
                        r=self.hparams.lora_r,
                        lora_alpha=self.hparams.lora_alpha,
                        lora_dropout=self.hparams.lora_dropout,
                        target_modules=['k_proj', 'v_proj', 'q_proj', 'o_proj',
                                        'gate_proj', 'up_proj', 'down_proj']
                    )
                self.peft_config = peft_config
                self.llama_model = get_peft_model(self.llama_model, peft_config)

            for _, param in self.llama_model.named_parameters():
                param.requires_grad = False
            self.llama_model.print_trainable_parameters()

        else:
            raise NotImplementedError()

        print('Loading LLAMA Done')

    def load_projector(self):
        name = self.hparams.model_name
        camel_name = ''.join([i.capitalize() for i in name.split('_')])

        try:
            Model = getattr(importlib.import_module('.' + name, package=__package__), camel_name)
        except Exception as e:
            raise ValueError(f'Invalid Module File Name or Invalid Class Name {name}.{camel_name}!') from e

        self.projector = self.instancialize(
            Model,
            rec_size=self.hparams.rec_size,
            llm_size=self.llama_model.config.hidden_size
        )

    def instancialize(self, Model, **other_args):
        class_args = inspect.getargspec(Model.__init__).args[1:]
        inkeys = self.hparams.keys()
        args1 = {}
        for arg in class_args:
            if arg in inkeys:
                args1[arg] = getattr(self.hparams, arg)
        args1.update(other_args)
        return Model(**args1)

    def load_rec_model(self, rec_model_path):
        print('Loading Rec Model')
        self.rec_model = torch.load(rec_model_path, map_location="cpu")
        self.rec_model.eval()
        for _, param in self.rec_model.named_parameters():
            param.requires_grad = False
        print('Loading Rec model Done')

    # -------------------- embedding injection --------------------
    def encode_items(self, seq):
        if self.hparams.rec_embed == "SASRec":
            item_rec_embs = self.rec_model.cacu_x(seq)
        elif self.hparams.rec_embed in ['Caser', 'GRU']:
            item_rec_embs = self.rec_model.item_embeddings(seq)
        else:
            raise ValueError(f"Unknown rec_embed: {self.hparams.rec_embed}")

        item_txt_embs = self.projector(item_rec_embs)
        return item_txt_embs

    def wrap_emb(self, batch):
        input_embeds = self.llama_model.get_input_embeddings()(batch["tokens"].input_ids)

        his_token_id = self.llama_tokenizer("[HistoryEmb]", return_tensors="pt",
                                            add_special_tokens=False).input_ids.item()
        cans_token_id = self.llama_tokenizer("[CansEmb]", return_tensors="pt",
                                             add_special_tokens=False).input_ids.item()
        item_token_id = self.llama_tokenizer("[ItemEmb]", return_tensors="pt",
                                             add_special_tokens=False).input_ids.item()

        his_item_embeds = self.encode_items(batch["seq"])
        cans_item_embeds = self.encode_items(batch["cans"])
        item_embeds = self.encode_items(batch["item_id"])

        for i in range(len(batch["len_seq"])):
            # history
            if (batch["tokens"].input_ids[i] == his_token_id).nonzero().shape[0] > 0:
                idx_tensor = (batch["tokens"].input_ids[i] == his_token_id).nonzero().view(-1)
                for idx, item_emb in zip(idx_tensor, his_item_embeds[i, :batch["len_seq"][i].item()]):
                    input_embeds[i, idx] = item_emb

            # candidates
            if (batch["tokens"].input_ids[i] == cans_token_id).nonzero().shape[0] > 0:
                idx_tensor = (batch["tokens"].input_ids[i] == cans_token_id).nonzero().view(-1)
                for idx, item_emb in zip(idx_tensor, cans_item_embeds[i, :batch["len_cans"][i].item()]):
                    input_embeds[i, idx] = item_emb

            # target item
            if (batch["tokens"].input_ids[i] == item_token_id).nonzero().shape[0] > 0:
                idx = (batch["tokens"].input_ids[i] == item_token_id).nonzero().item()
                input_embeds[i, idx] = item_embeds[i]

        return input_embeds